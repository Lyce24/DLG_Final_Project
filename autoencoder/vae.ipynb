{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (23544, 2792)\n",
      "Computed pos_weight shape: torch.Size([2792])\n",
      "Epoch [1/50] Train Loss: 2.6873 | Val Loss: 1.6549\n",
      "Epoch [2/50] Train Loss: 1.5861 | Val Loss: 1.6592\n",
      "Epoch [3/50] Train Loss: 1.3913 | Val Loss: 1.7426\n",
      "Epoch [4/50] Train Loss: 1.3299 | Val Loss: 1.5649\n",
      "Epoch [5/50] Train Loss: 1.2963 | Val Loss: 1.4908\n",
      "Epoch [6/50] Train Loss: 1.2690 | Val Loss: 1.6211\n",
      "Epoch [7/50] Train Loss: 1.2434 | Val Loss: 1.5015\n",
      "Epoch [8/50] Train Loss: 1.2401 | Val Loss: 1.5999\n",
      "Epoch [9/50] Train Loss: 1.2280 | Val Loss: 1.5353\n",
      "Epoch [10/50] Train Loss: 1.2297 | Val Loss: 1.6253\n",
      "Epoch [11/50] Train Loss: 1.2090 | Val Loss: 1.5282\n",
      "Epoch [12/50] Train Loss: 1.2114 | Val Loss: 1.5246\n",
      "Epoch [13/50] Train Loss: 1.2132 | Val Loss: 1.5964\n",
      "Epoch [14/50] Train Loss: 1.1923 | Val Loss: 1.6386\n",
      "Epoch [15/50] Train Loss: 1.2007 | Val Loss: 1.6030\n",
      "Epoch [16/50] Train Loss: 1.1927 | Val Loss: 1.6249\n",
      "Epoch [17/50] Train Loss: 1.1827 | Val Loss: 1.5376\n",
      "Epoch [18/50] Train Loss: 1.1770 | Val Loss: 1.6569\n",
      "Epoch [19/50] Train Loss: 1.1771 | Val Loss: 1.6460\n",
      "Epoch [20/50] Train Loss: 1.1781 | Val Loss: 1.6633\n",
      "Epoch [21/50] Train Loss: 1.1620 | Val Loss: 1.6786\n",
      "Epoch [22/50] Train Loss: 1.1464 | Val Loss: 1.6939\n",
      "Epoch [23/50] Train Loss: 1.1548 | Val Loss: 1.6858\n",
      "Epoch [24/50] Train Loss: 1.1398 | Val Loss: 1.7375\n",
      "Epoch [25/50] Train Loss: 1.1462 | Val Loss: 1.8424\n",
      "Epoch [26/50] Train Loss: 1.1243 | Val Loss: 1.7385\n",
      "Epoch [27/50] Train Loss: 1.1064 | Val Loss: 1.8775\n",
      "Epoch [28/50] Train Loss: 1.0889 | Val Loss: 1.8442\n",
      "Epoch [29/50] Train Loss: 1.0828 | Val Loss: 1.9157\n",
      "Epoch [30/50] Train Loss: 1.0778 | Val Loss: 1.6944\n",
      "Epoch [31/50] Train Loss: 1.0640 | Val Loss: 1.9561\n",
      "Epoch [32/50] Train Loss: 1.0594 | Val Loss: 1.9896\n",
      "Epoch [33/50] Train Loss: 1.0422 | Val Loss: 1.9076\n",
      "Epoch [34/50] Train Loss: 1.0370 | Val Loss: 1.7559\n",
      "Epoch [35/50] Train Loss: 1.0330 | Val Loss: 2.3589\n",
      "Epoch [36/50] Train Loss: 1.0241 | Val Loss: 1.9391\n",
      "Epoch [37/50] Train Loss: 1.0227 | Val Loss: 1.9532\n",
      "Epoch [38/50] Train Loss: 1.0212 | Val Loss: 2.1015\n",
      "Epoch [39/50] Train Loss: 1.0235 | Val Loss: 1.9841\n",
      "Epoch [40/50] Train Loss: 1.0115 | Val Loss: 1.9957\n",
      "Epoch [41/50] Train Loss: 1.0049 | Val Loss: 1.9842\n",
      "Epoch [42/50] Train Loss: 1.0006 | Val Loss: 2.0176\n",
      "Epoch [43/50] Train Loss: 0.9988 | Val Loss: 2.0473\n",
      "Epoch [44/50] Train Loss: 1.0072 | Val Loss: 2.9167\n",
      "Epoch [45/50] Train Loss: 0.9979 | Val Loss: 2.1485\n",
      "Epoch [46/50] Train Loss: 0.9857 | Val Loss: 2.3625\n",
      "Epoch [47/50] Train Loss: 0.9869 | Val Loss: 1.9320\n",
      "Epoch [48/50] Train Loss: 1.0007 | Val Loss: 2.1576\n",
      "Epoch [49/50] Train Loss: 0.9924 | Val Loss: 2.1069\n",
      "Epoch [50/50] Train Loss: 0.9883 | Val Loss: 2.0930\n",
      "Training complete.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MutationDataset.__init__() got an unexpected keyword argument 'noise_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 228\u001b[0m\n\u001b[1;32m    225\u001b[0m train_vae(model, train_loader, val_loader, pos_weight, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, kl_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Use full dataset for latent extraction (no noise)\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m full_dataset \u001b[38;5;241m=\u001b[39m MutationDataset(X, noise_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m    229\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    230\u001b[0m all_latents \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: MutationDataset.__init__() got an unexpected keyword argument 'noise_level'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1) LOAD & PREPROCESS DATA\n",
    "# ---------------------------------------------\n",
    "def load_mutation_data(csv_path):\n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Keep only the Patient column and the mutation columns (adjust column indices as needed)\n",
    "    df = df.iloc[:, [0] + list(range(263, len(df.columns) - 2))]\n",
    "    \n",
    "    # Extract patient IDs\n",
    "    patient_ids = df['Patient'].values\n",
    "    # Get binary mutation features; convert to float32 for PyTorch\n",
    "    X = df.drop(columns='Patient').values.astype(np.float32)\n",
    "    return X, patient_ids\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2) COMPUTE WEIGHTED LOSS FACTOR (POS_WEIGHT)\n",
    "# ---------------------------------------------\n",
    "def compute_pos_weight(X):\n",
    "    \"\"\"\n",
    "    Compute per-feature positive weight as (# negatives) / (# positives).\n",
    "    To avoid division by zero, add a small epsilon.\n",
    "    \"\"\"\n",
    "    eps = 1e-6\n",
    "    # X is (num_samples, num_features)\n",
    "    num_positives = np.sum(X, axis=0)\n",
    "    num_negatives = X.shape[0] - num_positives\n",
    "    pos_weight = num_negatives / (num_positives + eps)\n",
    "    return torch.tensor(pos_weight, dtype=torch.float32)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3) DEFINE THE VAE MODEL\n",
    "# ---------------------------------------------\n",
    "class MutationVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=64, dropout_p=0.5):\n",
    "        super(MutationVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # ENCODER\n",
    "        # reduce overfitting by adding dropout layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        # Two separate linear layers to output mean and log variance.\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # DECODER\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(512, input_dim)\n",
    "            # No Sigmoid here because BCEWithLogitsLoss applies sigmoid internally.\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        logits = self.decoder(z)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        logits = self.decode(z)\n",
    "        return logits, mu, logvar\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4) DEFINE A CUSTOM DATASET\n",
    "# ---------------------------------------------\n",
    "class MutationDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.from_numpy(X)  # shape: [num_samples, num_features]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Autoencoder: input is also target.\n",
    "        return self.X[idx], self.X[idx]\n",
    "\n",
    "def make_dataloaders(X, train_ratio=0.8, batch_size=32):\n",
    "    dataset = MutationDataset(X)\n",
    "    n = len(dataset)\n",
    "    n_train = int(train_ratio * n)\n",
    "    n_val = n - n_train\n",
    "    train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5) DEFINE THE VAE LOSS FUNCTION\n",
    "# ---------------------------------------------\n",
    "def vae_loss_function(logits, target, mu, logvar, pos_weight, kl_weight=0.001):\n",
    "    # Reconstruction loss: weighted BCEWithLogitsLoss (summed over batch and features)\n",
    "    bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='mean')\n",
    "    recon_loss = bce_loss(logits, target)\n",
    "    # KL divergence loss: sum over latent dimensions\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_weight * kl_loss\n",
    "\n",
    "\n",
    "def train_vae(model, train_loader, val_loader, pos_weight, \n",
    "                      num_epochs=50, lr=1e-3, device='cpu', kl_weight=0.001):\n",
    "    model.to(device)\n",
    "    # Use BCEWithLogitsLoss with the computed pos_weight.\n",
    "    # pos_weight should be of shape (input_dim,)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_target in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_target = batch_target.to(device)\n",
    "            # Forward pass: note that model returns logits.\n",
    "            logits, mu, logvar = model(batch_x)\n",
    "            \n",
    "            loss = vae_loss_function(logits, batch_target, mu, logvar, pos_weight.to(device), kl_weight)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * len(batch_x)\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_target in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_target = batch_target.to(device)\n",
    "                logits, mu, logvar = model(batch_x)\n",
    "                loss = vae_loss_function(logits, batch_target, mu, logvar, pos_weight.to(device), kl_weight)\n",
    "                val_loss += loss.item() * len(batch_x)\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6) VISUALIZE LATENT REPRESENTATIONS WITH t-SNE\n",
    "# ---------------------------------------------\n",
    "def visualize_latent(model, dataset, device='cpu'):\n",
    "    model.eval()\n",
    "    latent_list = []\n",
    "    with torch.no_grad():\n",
    "        loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "        for batch_x, _ in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            mu, _ = model.encode(batch_x)  # use mu as the latent representation\n",
    "            latent_list.append(mu.cpu().numpy())\n",
    "    latent_all = np.concatenate(latent_list, axis=0)\n",
    "    print(\"Latent shape:\", latent_all.shape)\n",
    "    \n",
    "    # Use t-SNE to reduce latent dimensions to 2D for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    latent_2d = tsne.fit_transform(latent_all)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(latent_2d[:, 0], latent_2d[:, 1], s=10, alpha=0.7)\n",
    "    plt.title(\"t-SNE Visualization of Latent Representations\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    # save plot\n",
    "    plt.savefig(\"../fig/latent_tsne.png\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7) MAIN: TRAIN MODEL, SAVE LATENT, VISUALIZE\n",
    "# ---------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    csv_path = \"../data/msk_2024_mutations_final.csv\"\n",
    "    X, patient_ids = load_mutation_data(csv_path)\n",
    "    print(\"Data shape:\", X.shape)\n",
    "\n",
    "    # Compute the positive weight per feature (for weighted loss)\n",
    "    pos_weight = compute_pos_weight(X)\n",
    "    print(\"Computed pos_weight shape:\", pos_weight.shape)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = make_dataloaders(X, train_ratio=0.8, batch_size=32)\n",
    "\n",
    "    # Initialize the autoencoder model\n",
    "    input_dim = X.shape[1]\n",
    "    latent_dim = 128  # Adjust latent dimension as needed\n",
    "    model = MutationVAE(input_dim=input_dim, latent_dim=latent_dim, dropout_p=0.5)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    train_vae(model, train_loader, val_loader, pos_weight, num_epochs=50, lr=1e-3, device=device, kl_weight=0.001)\n",
    "\n",
    "    # Use full dataset for latent extraction (no noise)\n",
    "    full_dataset = MutationDataset(X, noise_level=0.0)\n",
    "    model.eval()\n",
    "    all_latents = []\n",
    "    with torch.no_grad():\n",
    "        full_loader = DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
    "        for batch_x, _ in full_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            mu, _ = model.encode(batch_x)\n",
    "            all_latents.append(mu.cpu().numpy())\n",
    "    all_latents = np.concatenate(all_latents, axis=0)\n",
    "    \n",
    "    # Save latent representations along with patient IDs to CSV.\n",
    "    latent_cols = [f\"latent_{i}\" for i in range(latent_dim)]\n",
    "    latent_df = pd.DataFrame(all_latents, columns=latent_cols)\n",
    "    latent_df.insert(0, \"Patient\", patient_ids)\n",
    "    latent_df.to_csv(\"latent_representations.csv\", index=False)\n",
    "    print(\"Saved latent representations to 'gene_vae.csv'.\")\n",
    "\n",
    "    # Visualize the latent space using t-SNE.\n",
    "    visualize_latent(model, full_dataset, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci2470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
